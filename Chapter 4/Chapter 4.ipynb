{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the *bias term* (also called the *intercept term*).\n",
    "\n",
    "$$\\hat{y}=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$$\n",
    "\n",
    "- $\\hat{y}$ is the predicted value\n",
    "- $n$ is the number of features\n",
    "- $x_i$ is the $\\text{i}^{th}$ feature value\n",
    "- $\\theta_j$ is the $\\text{j}^{th}$ model parameter (including the bias term $\\theta_0$ and the feature weights $\\theta_1,\\theta_2,...,\\theta_n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in a more consice way: vectorized form\n",
    "$$\\hat{y}=h_\\theta(\\textbf{x})=\\theta^T\\cdot \\textbf{x}$$\n",
    "\n",
    "- $\\theta$ is the model's parameter vector (a colomn vector), containing the bias term $\\theta_0$ and the feature weights $\\theta_1$ to $\\theta_n$\n",
    "- $\\theta^T$ is the transpose of $\\theta$ (a row vector)\n",
    "- $\\textbf{x}$ is the instance's feature vector, containing $x_0$ to $x_n$, with $x_0$ always equal to 1\n",
    "- $\\theta^T\\cdot x$ is the dot product of $\\theta^T$ and $\\textbf{x}$\n",
    "- $h_\\theta$ is the hypothesis function, using the model parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MSE cost function**\n",
    "$$MSE(\\theta)=\\frac{1}{m}\\sum_{i=1}^m(\\theta^T\\cdot \\textbf{x}^{(i)}-y^{(i)})^2$$\n",
    "\n",
    "Compared with RMSE(root mean square error), it is simpler to minimize the mean square error (much easier to compute the gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation\n",
    "$$\\hat{\\theta}=(\\textbf{X}^T\\cdot \\textbf{X})^{-1}\\cdot\\textbf{X}^T\\cdot \\textbf{y}$$\n",
    "- $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function\n",
    "- $\\textbf{y}$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$\n",
    "\n",
    "**Computational complexity**  \n",
    "The Normal Equation computes the inverse of $\\textbf{X}^T\\cdot\\textbf{X}$, which is an $n\\times n$ matrix (where $n$ is the number of features). The computational complexity of inverting such a matrix is typically about $O(n^{2.4})$ to $O(n^3)$ (depending on the implementation).  \n",
    "Therefore, the Normal Equation gets very slow when the number of features grows large (e.g., 100,000).  \n",
    "    On the positive side, this equation is linear with regards to the number of instances in the training set($O(m)$), so it handles large training sets efficiently, provided they can fit in memory. Also, the computational complexity is linear with regards to both the number of instances you want to make predictions on and the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Gradient descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function.  \n",
    "Concretely, you start by filling $\\theta$ with random values (this is called *random initialization*), and then you improve it gradually, taking one little step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm *converges* to a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important parameter in gradient descent is the size of the steps, determined by the *learning rate* hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time. On the other hand, if the learning rate is too high, it might make the algorithm diverge, with larger and larger values, failing to find a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent pitfall**  \n",
    "Not all cost functions look like nice regular bowls. There may be holes, ridges, plateaus, and all sorts of irregular terrains, making convergence to the minimum very difficult.  \n",
    "Fotunately, the MSE cost function for a linear regression model happens to be a *convex funciton*, which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. These two facts have a great consequence: gradient descent is guaranteed to approach arbitrarily close the global minimum.  \n",
    "When using gradient descent, you should ensure that all features have a similar scale, or else it will take much longer to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Partial derivatives of the cost function*\n",
    "$$\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^m(\\theta^T\\cdot\\textbf{x}^{(i)}-y^{(i)})x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gradient vector of the cost function*\n",
    "$$\\nabla_\\theta MSE(\\theta)=\\begin{bmatrix}\\frac{\\partial}{\\partial\\theta_0}MSE(\\theta)\\\\\\frac{\\partial}{\\partial\\theta_1}MSE(\\theta)\\\\...\\\\\\frac{\\partial}{\\partial\\theta_n}MSE(\\theta)\\end{bmatrix}=\\frac{2}{m}\\textbf{X}^T\\cdot(\\textbf{X}\\cdot\\theta-\\textbf{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula involves calculations over the full training set $\\textbf{X}$, at each gradient descent step. This is why the algorithm is called batch gradient descent: it uses the whole batch of training data at every step. As a result it is terribly slow on very large training sets. However, gradient descent scales well with the number of features; training a linear regression model when there are hundreds of thousands of features is much faster using gradient descent than using the normal equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gradient descent step*\n",
    "$$\\theta^{(next step)}=\\theta-\\eta\\nabla_\\theta MSE(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a good learning rate, it's okay to use grid search. However, the number of iterations should be limited, or it may take too long to converge.  \n",
    "A simple solution is to set a very large number of iterations but to interrupt the algorithm when the gradient vector becomes tiny - that is, when its norm becomes smaller than a tiny number $\\epsilon$ (called the *tolerance*) - because this happens when gradient descent has (almost) reached the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convergence Rate**  \n",
    "When the cost function is convex and its slope does not change abruptly, it can be shown that batch gradient descent with a fixed learning rate has a *convergence rate* of $O(\\frac{1}{iteration})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance. This makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomness**  \n",
    "Instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm stops, the final parameter values are good, but not optimal.  \n",
    "When the cost function is very irregular, this can actually help the algorithm jump out of local minima, so stochastic gradient descent has a better chance of finding the global minimum than batch gradient descent does.  \n",
    "Randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution to this dilemma is to gradually reduce the learning rate. The steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum. This process is called *simulated annealing*. The function that determines the learning rate at each iteration is called the *learning schedule*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention we iterate by rounds of *m* iterations; each round is called an *epoch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step, instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called *mini- batches*. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comparison of algorithms for linear regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm | Large *m* | Out-of-core support | Large n | Hyperparameters | Scaling required | Scikit-learn |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Normal Equation | Fast | No | Slow | 0 | No | LinearRegression|\n",
    "| Batch GD | Slow | No | Fast | 2 | Yes | n/a |\n",
    "| Stochastic GD | Fast | Yes | Fast | $\\ge$ 2 | Yes | SGDRegressor |\n",
    "| Mini-batch GD | Fast | Yes | Fast | $\\ge$ 2 | Yes | n/a |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called *polynomial regression*.  \n",
    "    When there are multiple features, polynomial regression is capable of finding relationships between features (which is something a plain linear regression model cannot do). This is made possible by the fact that `PolynomialFeatures` also adds all combinations of features up to the given degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Curve**  \n",
    "Apart from cross-validation, another way to get an estimate of a model's generalization performance is to look at the learning curves: these are plots of the model's performance on the training set and the validation set as a function of the training set size. To generate the plots, simply train the model several times on different sized subsets of the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Bias/Variance Tradeoff**  \n",
    "A model's generalization error can be expressed as the sum of three very different errors:  \n",
    "*Bias*  \n",
    "This part of the generalization error is due to wrong assumptions, such as assumpting that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.  \n",
    "*Variance*  \n",
    "This part is due to the model’s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance, and thus to overfit the training data.  \n",
    "*Irreducible error*  \n",
    "This part is due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers).  \n",
    "Increasing a model's complexity will typically increase its variance and reduce its bias. Conversely, reducing a model's complexity increases its bias and reduces its variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression (also called *Tikhonov regularization*) is a regularized version of linear regression: a *regularized term* equal to $\\alpha\\sum_{i=1}^n\\theta_i^2$ is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter $\\alpha$ controls how much you want to regularize the model. If $\\alpha=0$ then ridge regression is just linear regression. If $\\alpha$ is very large, then all weights end up very close to zero and the result is a flat line going through the data's mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression cost function: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta)=MSE(\\theta)+\\alpha\\frac{1}{2}\\sum_{i=1}^n\\theta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the bias term $\\theta_0$ is not regularized. If we define $\\textbf{w}$ as the vector of feature weights ($\\theta_1$ to $\\theta_n$), then the regularization term is simply equal to $\\frac{1}{2}(||\\textbf{w}||_2)^2$, where $||\\cdot||_2$ represents the $\\ell_2$ norm of the weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression closed-form: $\\hat{\\theta}=(\\textbf{X}^T\\cdot\\textbf{X}+\\alpha\\textbf{A})^{-1}\\cdot\\textbf{X}^T\\cdot\\textbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least absolute shrinkage and selection operator regression (simply called Lasso regression) is another regularized version of linear regression: just like ridge regression, it adds a regularization term to the cost function, but it use the $\\ell_1$ norm of the weight vector instead of half the square of the $\\ell_2$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta)=MSE(\\theta)+\\alpha\\sum_{i=1}^n|\\theta_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important characteristic of Lasso regression is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero). Lasso regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic net is a middle ground between ridge regression and lasso regression. The regularization term is a simple mix of both ridge and lasso’s regularization terms, and you can control the mix ratio *r*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic net cost function:\n",
    "$$J(\\theta)=MSE(\\theta)+r\\alpha\\sum_{i=1}^n|\\theta_i|+\\frac{1-r}{2}\\alpha\\sum_{i=1}^n\\theta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very different way to regularize iterative learning algorithms such as gradient descent is to stop training as soon as the validation error reaches a minimum. This is called *early stopping*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Stochastic and Mini-batch gradient descent, the learning curves are not so smooth as batch gradient, and it may be hard to know whether you have reached the minimum or not. One solution is to stop only after the validation error has been above the minimum for some time, then roll back the model parameters to the point where the validation error was at a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is commonly used to estimate the probability that an instance belongs to a particular class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression model estimated probability\n",
    "$$\\hat{p}=h_{\\theta}(\\textbf{x})=\\sigma(\\theta^T\\cdot\\textbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic function\n",
    "$$\\sigma(t)=\\frac{1}{1+exp(-t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression cost function\n",
    "$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^m[y^{(i)}log(\\hat{p}^{(i)})+(1-y^{(i)})log(1-\\hat{p}^{(i)})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic cost function partial derivatives\n",
    "$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m(\\sigma(\\theta^T\\cdot\\textbf{x}^{(i)})-y^{(i)})x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the other linear methods, logistic regression models can be regularized using $\\ell_1$ or $\\ell_2$ penalties. Scikit-learn adds an $\\ell_2$ penalty by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers. This is called softmax regression, or multinomial logistic regression.  \n",
    "The idea is: when given an instance $\\textbf{x}$, the softmax regression model first computes a score $s_k(\\textbf{x})$ for each class *k*, then estimates the probability of each class by applying the softmax function (also called the *normalized exponential*) to the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax score for class k\n",
    "$$s_k(\\textbf{x})=\\theta_k^T\\cdot\\textbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have computed the score of every class for the instance $\\textbf{x}$, you can estimate the probability $\\hat{p_k}$ that the instance belongs to class *k* by running the scores through the softmax function: it computes the exponential of every score, then normalizes them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax function\n",
    "$$\\hat{p_k}=\\sigma(\\textbf{s}(\\textbf{x})_k)=\\frac{\\exp(s_k(\\textbf{x}))}{\\sum_{j=1}^K\\exp(s_j(\\textbf{x}))}$$\n",
    "\n",
    "- *K* is the number of classes\n",
    "- $\\textbf{s}(\\textbf{x})$ is a vector containing the scores of each class for the instance $\\textbf{x}$\n",
    "- $\\sigma(\\textbf{s}(\\textbf{x}))_k$ is the estimated probability that the instance $\\textbf{x}$ belongs to class *k* given the scores of each class for that instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax regression classifier prediction\n",
    "$$\\hat{y}=\\arg\\max_k\\sigma(\\textbf{s}(\\textbf{x}))_k=\\arg\\max_k s_k(\\textbf{x})=\\arg\\max_k(\\theta_k^T\\cdot\\textbf{x})$$\n",
    "\n",
    "The *argmax* operator returns the value of a variable that maximizes a function. In this equation, it returns the value of *k* that maximizes the estimated probability $\\sigma(\\textbf{s}(\\textbf{x}))_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy cost function\n",
    "$$J(\\Theta)=-\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^Ky_k^{(i)}\\log(\\hat{p_k}^{(i)})$$\n",
    "\n",
    "$y_k^{(i)}$ is equal to 1 if the target class for the $i^{th}$ instance is *k*; otherwise, it is equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy gradient vector for class k\n",
    "$$\\nabla_{\\theta_k}J(\\Theta)=\\frac{1}{m}\\sum_{i=1}^m(\\hat{p_k}^{(i)}-y_k^{(i)})\\textbf{x}^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
