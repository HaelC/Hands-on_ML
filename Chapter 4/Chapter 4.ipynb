{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the *bias term* (also called the *intercept term*).\n",
    "\n",
    "$$\\hat{y}=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$$\n",
    "\n",
    "- $\\hat{y}$ is the predicted value\n",
    "- $n$ is the number of features\n",
    "- $x_i$ is the $\\text{i}^{th}$ feature value\n",
    "- $\\theta_j$ is the $\\text{j}^{th}$ model parameter (including the bias term $\\theta_0$ and the feature weights $\\theta_1,\\theta_2,...,\\theta_n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in a more consice way: vectorized form\n",
    "$$\\hat{y}=h_\\theta(\\textbf{x})=\\theta^T\\cdot \\textbf{x}$$\n",
    "\n",
    "- $\\theta$ is the model's parameter vector (a colomn vector), containing the bias term $\\theta_0$ and the feature weights $\\theta_1$ to $\\theta_n$\n",
    "- $\\theta^T$ is the transpose of $\\theta$ (a row vector)\n",
    "- $\\textbf{x}$ is the instance's feature vector, containing $x_0$ to $x_n$, with $x_0$ always equal to 1\n",
    "- $\\theta^T\\cdot x$ is the dot product of $\\theta^T$ and $\\textbf{x}$\n",
    "- $h_\\theta$ is the hypothesis function, using the model parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MSE cost function**\n",
    "$$MSE(\\theta)=\\frac{1}{m}\\sum_{i=1}^m(\\theta^T\\cdot \\textbf{x}^{(i)}-y^{(i)})^2$$\n",
    "\n",
    "Compared with RMSE(root mean square error), it is simpler to minimize the mean square error (much easier to compute the gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation\n",
    "$$\\hat{\\theta}=(\\textbf{X}^T\\cdot \\textbf{X})^{-1}\\cdot\\textbf{X}^T\\cdot \\textbf{y}$$\n",
    "- $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function\n",
    "- $\\textbf{y}$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$\n",
    "\n",
    "**Computational complexity**  \n",
    "The Normal Equation computes the inverse of $\\textbf{X}^T\\cdot\\textbf{X}$, which is an $n\\times n$ matrix (where $n$ is the number of features). The computational complexity of inverting such a matrix is typically about $O(n^{2.4})$ to $O(n^3)$ (depending on the implementation).  \n",
    "Therefore, the Normal Equation gets very slow when the number of features grows large (e.g., 100,000).  \n",
    "    On the positive side, this equation is linear with regards to the number of instances in the training set($O(m)$), so it handles large training sets efficiently, provided they can fit in memory. Also, the computational complexity is linear with regards to both the number of instances you want to make predictions on and the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Gradient descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
